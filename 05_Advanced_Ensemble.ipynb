{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ† ê³ ê¸‰ ì•™ìƒë¸” ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "**ì‹¤í–‰ í™˜ê²½**: Google Colab (GPU í•„ìˆ˜)\n",
    "\n",
    "**ì‹¤í–‰ ìˆœì„œ**:\n",
    "1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "2. ë°ì´í„° ì—…ë¡œë“œ (Colabì— data í´ë” ì—…ë¡œë“œ)\n",
    "3. ì „ì²˜ë¦¬\n",
    "4. ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ\n",
    "   - 5ê°œ ì‹œë“œ ê±´ë¬¼ë³„ TabPFN\n",
    "   - AutoGluon ëª¨ë¸\n",
    "   - ê±´ë¬¼ íƒ€ì…ë³„ TabPFN\n",
    "5. ìµœì¢… ì•™ìƒë¸”\n",
    "\n",
    "**ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„**: ì•½ 5-8ì‹œê°„ (GPU ì‚¬ìš© ì‹œ)\n",
    "\n",
    "**ì°¸ê³ **: 1ë“± íŒ€ì˜ ì „ì²´ ì•™ìƒë¸”ì„ ì¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Section 0: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë¨¼ì € êµ¬ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•´ Numpyì™€ Pandas íŠ¹ì • ë²„ì „ì„ ê°•ì œ ì„¤ì¹˜\n",
    "!pip install \"numpy==1.26.4\" \"pandas==2.1.4\" \"scikit-learn==1.5.2\"\n",
    "\n",
    "# 2. ê·¸ ë‹¤ìŒ AutoGluonê³¼ TabPFN ì„¤ì¹˜ (ì˜ì¡´ì„± ìë™ ì—…ê·¸ë ˆì´ë“œ ë°©ì§€ ì˜µì…˜ ì¶”ê°€)\n",
    "!pip install autogluon tabpfn==2.1.2 tabpfn-extensions==0.1.3 --no-build-isolation\n",
    "\n",
    "# 3. ì„¤ì¹˜ ì™„ë£Œ í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘ì„ ì½”ë“œë¡œ ì²˜ë¦¬ (ìë™ìœ¼ë¡œ ì„¸ì…˜ì´ ì£½ì—ˆë‹¤ ì‚´ì•„ë‚  ìˆ˜ ìˆìŒ)\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabpfn import TabPFNRegressor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëœë¤ ì‹œë“œ ê³ ì • í•¨ìˆ˜\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "print(\"âœ… ê¸°ë³¸ ëœë¤ ì‹œë“œ ì„¤ì •: 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (Colabì—ì„œëŠ” data í´ë”ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”)\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# ë°ì´í„° í´ë” êµ¬ì¡° í™•ì¸\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"âœ… ë°ì´í„° í´ë” ë°œê²¬\")\n",
    "    print(\"\\ní´ë” ë‚´ìš©:\")\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. data í´ë”ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
    "    print(\"\\ní•„ìš”í•œ íŒŒì¼:\")\n",
    "    print(\"  - train.csv\")\n",
    "    print(\"  - test.csv\")\n",
    "    print(\"  - building_info.csv\")\n",
    "    print(\"  - sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 1: ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ (First_Place_Pipelineê³¼ ë™ì¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìˆ˜ ì •ì˜\n",
    "WEEK_H = 168  # 1ì£¼ì¼ = 168ì‹œê°„\n",
    "EPS = 1e-3\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë§¤í•‘\n",
    "TRAIN_COL_RENAMES = {\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ì¼ì‹œ': 'date_time',\n",
    "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
    "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
    "    'í’ì†(m/s)': 'windspeed',\n",
    "    'ìŠµë„(%)': 'humidity',\n",
    "    'ì¼ì¡°(hr)': 'sunshine',\n",
    "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
    "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
    "}\n",
    "TEST_COL_RENAMES = TRAIN_COL_RENAMES.copy()\n",
    "\n",
    "BUILDING_INFO_RENAMES = {\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ê±´ë¬¼ìœ í˜•': 'building_type',\n",
    "    'ì—°ë©´ì (m2)': 'total_area',\n",
    "    'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',\n",
    "    'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'solar_power_capacity',\n",
    "    'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',\n",
    "    'PCSìš©ëŸ‰(kW)': 'pcs_capacity'\n",
    "}\n",
    "\n",
    "TYPE_TRANSLATION = {\n",
    "    'ê±´ë¬¼ê¸°íƒ€': 'Other Buildings',\n",
    "    'ê³µê³µ': 'Public',\n",
    "    'í•™êµ': 'School',\n",
    "    'ë°±í™”ì ': 'Department Store',\n",
    "    'ë³‘ì›': 'Hospital',\n",
    "    'ìƒìš©': 'Commercial',\n",
    "    'ì•„íŒŒíŠ¸': 'Apartment',\n",
    "    'ì—°êµ¬ì†Œ': 'Research Institute',\n",
    "    'í˜¸í…”': 'Hotel',\n",
    "    'IDC(ì „í™”êµ­)': 'IDC'\n",
    "}\n",
    "\n",
    "KR_HOLIDAYS_2024 = {\"2024-06-06\", \"2024-08-15\"}\n",
    "\n",
    "DROP_COLS = ['sunshine', 'solar_radiation', 'solar_power_capacity', 'ess_capacity', 'pcs_capacity', \n",
    "             'hour', 'day_of_week', 'day_of_year']\n",
    "CAT_COLS = ['building_type', 'building_number']\n",
    "\n",
    "print(\"âœ… ìƒìˆ˜ ë° ë§¤í•‘ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_raw(data_dir: str = \"./data\"):\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train.csv'), encoding='utf-8-sig')\n",
    "    test = pd.read_csv(os.path.join(data_dir, 'test.csv'), encoding='utf-8-sig')\n",
    "    info = pd.read_csv(os.path.join(data_dir, 'building_info.csv'), encoding='utf-8-sig')\n",
    "    return train, test, info\n",
    "\n",
    "def rename_columns(df: pd.DataFrame, mapping: dict):\n",
    "    df = df.rename(columns=mapping)\n",
    "    if 'num_date_time' in df.columns:\n",
    "        df = df.drop('num_date_time', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_building_info(info: pd.DataFrame) -> pd.DataFrame:\n",
    "    info = info.rename(columns=BUILDING_INFO_RENAMES)\n",
    "    info['building_type'] = info['building_type'].replace(TYPE_TRANSLATION)\n",
    "    return info\n",
    "\n",
    "def merge_datasets(train: pd.DataFrame, test: pd.DataFrame, info: pd.DataFrame):\n",
    "    train = train.merge(info, on='building_number', how='left')\n",
    "    test = test.merge(info, on='building_number', how='left')\n",
    "    return train, test\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering í•¨ìˆ˜ë“¤\n",
    "def create_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d %H')\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['day_of_year'] = df['date_time'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def add_summer_cycle_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    start_date = datetime.strptime(\"2024-05-20 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2024-09-08 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    period_seconds = (end_date - start_date).total_seconds()\n",
    "    \n",
    "    def summer_cos(date):\n",
    "        return np.cos(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "    \n",
    "    def summer_sin(date):\n",
    "        return np.sin(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "        \n",
    "    df_copy['summer_cos'] = df_copy['date_time'].apply(summer_cos)\n",
    "    df_copy['summer_sin'] = df_copy['date_time'].apply(summer_sin)\n",
    "    return df_copy\n",
    "\n",
    "def add_squared_features(df: pd.DataFrame, target_cols: List[str] = ['temperature', 'humidity']) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    for col in target_cols:\n",
    "        df_copy[f'{col}_squared'] = df_copy[col] ** 2\n",
    "    return df_copy\n",
    "\n",
    "def create_cyclic_features(df):\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['sin_doy'] = np.sin(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    df['cos_doy'] = np.cos(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    return df\n",
    "\n",
    "def cooling_degree_hour(temperature, window=12, base_temp=26):\n",
    "    cdhs = []\n",
    "    temps = temperature.values\n",
    "    for i in range(len(temps)):\n",
    "        if i < window:\n",
    "            cdh = np.sum(np.maximum(temps[:i+1] - base_temp, 0))\n",
    "        else:\n",
    "            cdh = np.sum(np.maximum(temps[i-window+1:i+1] - base_temp, 0))\n",
    "        cdhs.append(cdh)\n",
    "    return cdhs\n",
    "\n",
    "def add_cdh_feature(df: pd.DataFrame, window: int = 12, base_temp: float = 26.0) -> pd.DataFrame:\n",
    "    cdhs_all = []\n",
    "    for b in df['building_number'].unique():\n",
    "        temps = df.loc[df['building_number'] == b, 'temperature']\n",
    "        cdhs_all.extend(cooling_degree_hour(temps, window=window, base_temp=base_temp))\n",
    "    df['CDH'] = cdhs_all\n",
    "    return df\n",
    "\n",
    "def add_cdd_feature(df: pd.DataFrame, base_temp: float = 18.0, window: int = 24) -> pd.DataFrame:\n",
    "    df['excess'] = (df['temperature'] - base_temp).clip(lower=0)\n",
    "    df['CDD'] = df.groupby('building_number')['excess'].transform(\n",
    "        lambda s: s.rolling(window, min_periods=1).sum()\n",
    "    )\n",
    "    df.drop(columns=['excess'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_thi_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['THI'] = (9/5 * df['temperature'] \n",
    "                 - 0.55 * (1 - df['humidity']/100) \n",
    "                 * (9/5 * df['temperature'] - 26) \n",
    "                 + 32)\n",
    "    return df\n",
    "\n",
    "def add_wct_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    v16 = df['windspeed'] ** 0.16\n",
    "    df['WCT'] = (13.12 \n",
    "                 + 0.6215 * df['temperature'] \n",
    "                 - 11.37 * v16 \n",
    "                 + 0.3965 * v16 * df['temperature'])\n",
    "    return df\n",
    "\n",
    "def add_temp_features(data):\n",
    "    avg_temp = pd.pivot_table(\n",
    "        data[data['hour'] % 3 == 0],\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='mean'\n",
    "    ).reset_index().rename(columns={'temperature': 'avg_temp'})\n",
    "    data = pd.merge(data, avg_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    max_temp = pd.pivot_table(\n",
    "        data,\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='max'\n",
    "    ).reset_index().rename(columns={'temperature': 'max_temp'})\n",
    "    data = pd.merge(data, max_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    min_temp = pd.pivot_table(\n",
    "        data,\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='min'\n",
    "    ).reset_index().rename(columns={'temperature': 'min_temp'})\n",
    "    data = pd.merge(data, min_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    data['temp_diff'] = data['max_temp'] - data['min_temp']\n",
    "    return data\n",
    "\n",
    "def add_humid_features(data):\n",
    "    avg_humid = pd.pivot_table(\n",
    "        data[data['hour'] % 3 == 0],\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='mean'\n",
    "    ).reset_index().rename(columns={'humidity': 'avg_humid'})\n",
    "    data = pd.merge(data, avg_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    max_humid = pd.pivot_table(\n",
    "        data,\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='max'\n",
    "    ).reset_index().rename(columns={'humidity': 'max_humid'})\n",
    "    data = pd.merge(data, max_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    min_humid = pd.pivot_table(\n",
    "        data,\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='min'\n",
    "    ).reset_index().rename(columns={'humidity': 'min_humid'})\n",
    "    data = pd.merge(data, min_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    data['humid_diff'] = data['max_humid'] - data['min_humid']\n",
    "    return data\n",
    "\n",
    "def _prep(df, time_col, group_col):\n",
    "    return df.sort_values([group_col, time_col])\n",
    "\n",
    "def add_weekly_slope(df: pd.DataFrame, time_col: str = 'date_time',\n",
    "                     group_col: str = 'building_number',\n",
    "                     power_col: str = 'power_consumption',\n",
    "                     lookback: int = 6) -> pd.DataFrame:\n",
    "    df = _prep(df, time_col, group_col)\n",
    "\n",
    "    def _beta(x: pd.Series) -> float:\n",
    "        if x.isna().any(): \n",
    "            return np.nan\n",
    "        idx = np.arange(len(x))\n",
    "        num = idx.dot(x) * len(x) - idx.sum() * x.sum()\n",
    "        den = len(x) * (idx**2).sum() - idx.sum()**2\n",
    "        return num / den if den else 0.0\n",
    "\n",
    "    pw_seq = df.groupby(group_col)[power_col].shift(WEEK_H)\n",
    "    col = f'power_week_slope{lookback}h'\n",
    "    df[col] = pw_seq.groupby(df[group_col]).transform(\n",
    "        lambda s: s.rolling(lookback).apply(_beta, raw=False)\n",
    "    ).fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Feature Engineering í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íœ´ì¼ ì„¤ì • í•¨ìˆ˜ (ê±´ë¬¼ë³„ ì„¸ë°€í•œ ì„¤ì •)\n",
    "def _ensure_dt(df):\n",
    "    if not np.issubdtype(df[\"date_time\"].dtype, np.datetime64):\n",
    "        df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    return df\n",
    "\n",
    "def _nth_weekday_in_month(series_dt, weekday_target):\n",
    "    first_of_month = series_dt.values.astype(\"datetime64[M]\").astype(\"datetime64[ns]\")\n",
    "    first_weekday = pd.to_datetime(first_of_month).weekday\n",
    "    weekday = series_dt.dt.weekday.values\n",
    "    day = series_dt.dt.day.values\n",
    "    first_occ_day = 1 + ((weekday_target - first_weekday) % 7)\n",
    "    nth = ((day - first_occ_day) // 7) + 1\n",
    "    nth = np.where(day >= first_occ_day, nth, 0)\n",
    "    return nth\n",
    "\n",
    "def add_holiday(df: pd.DataFrame, kr_holidays: set[str] = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "    if kr_holidays is None:\n",
    "        kr_holidays = KR_HOLIDAYS_2024\n",
    "\n",
    "    df[\"weekday\"] = df[\"date_time\"].dt.weekday\n",
    "    df[\"date\"] = df[\"date_time\"].dt.date\n",
    "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "    df[\"holiday\"] = 0\n",
    "\n",
    "    is_kr = df[\"date\"].astype(str).isin(kr_holidays).values\n",
    "    bt = df[\"building_type\"]\n",
    "\n",
    "    # Apartment: í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Apartment\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # Hospital: ì£¼ë§ or ê³µíœ´ì¼ íœ´ì‹\n",
    "    mm = bt == \"Hospital\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # Public: ê¸°ë³¸ ì£¼ë§ or ê³µíœ´ì¼, ë‹¨ 33/92ëŠ” í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Public\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "        mm_always_open = df[\"building_number\"].isin([33, 92])\n",
    "        df.loc[mm_always_open, \"holiday\"] = 0\n",
    "\n",
    "    # Hotel: í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Hotel\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # School: ì£¼ë§ or ê³µíœ´ì¼ íœ´ì‹\n",
    "    mm = bt == \"School\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # IDC: ê°œë³„ ê·œì¹™\n",
    "    mm_idc = bt == \"IDC\"\n",
    "    if mm_idc.any():\n",
    "        ids = [36, 43, 52]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "        mmx = df[\"building_number\"].eq(64)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        mmx = df[\"building_number\"].eq(67)\n",
    "        if mmx.any():\n",
    "            df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "            df.loc[mmx & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "\n",
    "    # Commercial: ê°œë³„ ê·œì¹™\n",
    "    mm = bt == \"Commercial\"\n",
    "    if mm.any():\n",
    "        mmx = df[\"building_number\"].eq(2)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        ids = [6, 16, 20, 51, 86]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "\n",
    "    # Other Buildings\n",
    "    mmx = df[\"building_number\"].eq(26)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].isin([0, 1]).astype(int)\n",
    "    mmx = df[\"building_number\"].eq(82)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(0).astype(int)\n",
    "    mmx = df[\"building_number\"].isin([47, 69])\n",
    "    df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "    mmx = df[\"building_number\"].eq(97)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(5).astype(int)\n",
    "\n",
    "    # Department Store: ê°œë³„ íœ´ì¼ ê·œì¹™\n",
    "    mm = bt == \"Department Store\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = 0\n",
    "        nth_sun = _nth_weekday_in_month(df[\"date_time\"], 6)\n",
    "        nth_mon = _nth_weekday_in_month(df[\"date_time\"], 0)\n",
    "\n",
    "        def mark_nth_weekday(building, weekday, nth_set):\n",
    "            if weekday == 6:\n",
    "                nth = nth_sun\n",
    "            elif weekday == 0:\n",
    "                nth = nth_mon\n",
    "            else:\n",
    "                nth = _nth_weekday_in_month(df[\"date_time\"], weekday)\n",
    "            sel = df[\"building_number\"].eq(building) & df[\"weekday\"].eq(weekday) & pd.Series(nth).isin(list(nth_set)).values\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        df.loc[df[\"building_number\"].eq(18) & df[\"weekday\"].eq(6), \"holiday\"] = 1\n",
    "\n",
    "        special = {\n",
    "            19: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            45: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            54: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            74: [\"2024-06-17\", \"2024-07-01\"],\n",
    "            79: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            95: [\"2024-07-08\", \"2024-08-05\"],\n",
    "            29: [\"2024-06-10\", \"2024-07-10\", \"2024-08-10\"],\n",
    "        }\n",
    "        for b, dates in special.items():\n",
    "            sel = df[\"building_number\"].eq(b) & df[\"date\"].astype(str).isin(dates)\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        mark_nth_weekday(27, 6, {2, 4})\n",
    "        mark_nth_weekday(29, 6, {4})\n",
    "        mark_nth_weekday(32, 0, {2, 4})\n",
    "        for b in [40, 59, 63]:\n",
    "            mark_nth_weekday(b, 6, {2, 4})\n",
    "\n",
    "    # Research Institute\n",
    "    mm = bt == \"Research Institute\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "        \n",
    "        nth_fri = _nth_weekday_in_month(df[\"date_time\"], 4)\n",
    "        sel_23 = df[\"building_number\"].eq(23) & mm\n",
    "        df.loc[sel_23 & df[\"weekday\"].eq(4) & pd.Series(nth_fri).eq(3).values, \"holiday\"] = 1\n",
    "        extra_23 = {\"2024-06-07\", \"2024-08-16\"}\n",
    "        df.loc[sel_23 & df[\"date\"].astype(str).isin(extra_23), \"holiday\"] = 1\n",
    "\n",
    "        sel_49 = df[\"building_number\"].eq(49) & mm\n",
    "        df.loc[sel_49 & df[\"date\"].astype(str).eq(\"2024-08-22\"), \"holiday\"] = 1\n",
    "\n",
    "        sel_53 = df[\"building_number\"].eq(53) & mm\n",
    "        extra_53 = {\"2024-06-15\", \"2024-06-16\"}\n",
    "        df.loc[sel_53 & df[\"date\"].astype(str).isin(extra_53), \"holiday\"] = 1\n",
    "\n",
    "        sel_94 = df[\"building_number\"].eq(94) & mm\n",
    "        extra_94 = {\"2024-06-07\", \"2024-08-16\"}\n",
    "        df.loc[sel_94 & df[\"date\"].astype(str).isin(extra_94), \"holiday\"] = 1\n",
    "\n",
    "    df.loc[(df[\"building_number\"].eq(67)) & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"âœ… íœ´ì¼ ì„¤ì • í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ìƒì¹˜ ì œê±° ë° íƒ€ê²Ÿ í†µê³„ í•¨ìˆ˜\n",
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "\n",
    "    rules_lt = [\n",
    "        (25, 0, \"eq\"), (70, 200, \"lt\"),\n",
    "        (44, 800, \"lt\"), (90, 800, \"lt\"), (42, 2000, \"lt\"), (17, 1000, \"lt\"),\n",
    "        (68, 600, \"lt\"), (72, 600, \"lt\"), (80, 600, \"lt\"), (92, 200, \"lt\"),\n",
    "        (98, 500, \"lt\"),\n",
    "        (97, 500, \"lt\"), (78, 400, \"lt\"), (26, 300, \"lt\"), (7, 2000, \"lt\"),\n",
    "        (76, 2000, \"lt\"), (41, 2200, \"lt\"), (20, 1600, \"lt\"),\n",
    "        (5, 2000, \"lt\"), (8, 250, \"lt\"), (12, 3500, \"lt\"),\n",
    "        (67, 7333, \"lt\"), (81, 800, \"lt\"), (52, 2000, \"lt\"), (43, 6000, \"lt\"), (30, 8000, \"lt\"),\n",
    "    ]\n",
    "\n",
    "    mask_ok = pd.Series(True, index=df.index)\n",
    "    pc = df[\"power_consumption\"]\n",
    "    bnum = df[\"building_number\"]\n",
    "\n",
    "    for bn, th, op in rules_lt:\n",
    "        if op == \"lt\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc < th))\n",
    "        elif op == \"eq\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc == th))\n",
    "\n",
    "    mask_ok &= ~((bnum.eq(10)) & (df[\"date_time\"].between(pd.Timestamp(\"2024-07-05\"), pd.Timestamp(\"2024-08-22\"))))\n",
    "    mask_ok &= ~((bnum.eq(57)) & (df[\"date_time\"] < pd.Timestamp(\"2024-06-07\")))\n",
    "    mask_ok &= ~((bnum.eq(94)) & (df[\"date_time\"].between(pd.Timestamp(\"2024-07-27 09:00\"), pd.Timestamp(\"2024-08-04 23:00\"))))\n",
    "    mask_ok &= ~((bnum.eq(53)) & (df[\"date_time\"].dt.normalize().isin([pd.Timestamp(\"2024-06-15\"), pd.Timestamp(\"2024-06-16\")])))\n",
    "    mask_ok &= ~((bnum.eq(53)) & (df[\"date_time\"] >= pd.Timestamp(\"2024-08-17\")) & (pc <= 1000))\n",
    "\n",
    "    return df.loc[mask_ok].reset_index(drop=True)\n",
    "\n",
    "def mean_std_power(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    is_train = df['power_consumption'].notna()\n",
    "\n",
    "    dt = pd.to_datetime(df['date_time'])\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = dt.dt.date\n",
    "    if 'hour' not in df.columns:\n",
    "        df['hour'] = dt.dt.hour\n",
    "    if 'day_of_week' not in df.columns:\n",
    "        df['day_of_week'] = dt.dt.weekday\n",
    "    if 'month' not in df.columns:\n",
    "        df['month'] = dt.dt.month\n",
    "\n",
    "    df['holiday'] = df['holiday'].fillna(0).astype(int)\n",
    "\n",
    "    base_ratio = np.array([1.0] * 7)\n",
    "    ratio_all = base_ratio - 0\n",
    "    df.loc[is_train, 'power_consumption'] = df.loc[is_train].apply(\n",
    "        lambda r: r['power_consumption'] * ratio_all[int(r['day_of_week'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    train_df = df[is_train].copy()\n",
    "\n",
    "    PUBLIC_HOLS = {\"2024-06-06\", \"2024-08-15\"}\n",
    "    train_df_dow = train_df[~train_df['date'].astype(str).isin(PUBLIC_HOLS)].copy()\n",
    "\n",
    "    dow_hour_mean = train_df_dow.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].mean().reset_index(name='dow_hour_mean')\n",
    "    dow_hour_std = train_df_dow.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].std().reset_index(name='dow_hour_std')\n",
    "    df = df.merge(dow_hour_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "    df = df.merge(dow_hour_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "    hol_mean = train_df.groupby(['building_number', 'hour', 'holiday'])['power_consumption'].mean().reset_index(name='holiday_mean')\n",
    "    hol_std = train_df.groupby(['building_number', 'hour', 'holiday'])['power_consumption'].std().reset_index(name='holiday_std')\n",
    "    df = df.merge(hol_mean, on=['building_number', 'hour', 'holiday'], how='left')\n",
    "    df = df.merge(hol_std, on=['building_number', 'hour', 'holiday'], how='left')\n",
    "\n",
    "    hr_mean = train_df.groupby(['building_number', 'hour'])['power_consumption'].mean().reset_index(name='hour_mean')\n",
    "    hr_std = train_df.groupby(['building_number', 'hour'])['power_consumption'].std().reset_index(name='hour_std')\n",
    "    df = df.merge(hr_mean, on=['building_number', 'hour'], how='left')\n",
    "    df = df.merge(hr_std, on=['building_number', 'hour'], how='left')\n",
    "\n",
    "    mh_mean = train_df.groupby(['building_number', 'month', 'hour'])['power_consumption'].mean().reset_index(name='month_hour_mean')\n",
    "    mh_std = train_df.groupby(['building_number', 'month', 'hour'])['power_consumption'].std().reset_index(name='month_hour_std')\n",
    "    df = df.merge(mh_mean, on=['building_number', 'month', 'hour'], how='left')\n",
    "    df = df.merge(mh_std, on=['building_number', 'month', 'hour'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"âœ… ì´ìƒì¹˜ ì œê±° ë° íƒ€ê²Ÿ í†µê³„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Section 2: ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "print(\"ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "train, test, info = load_raw(DATA_DIR)\n",
    "train = rename_columns(train, TRAIN_COL_RENAMES)\n",
    "test = rename_columns(test, TEST_COL_RENAMES)\n",
    "info = preprocess_building_info(info)\n",
    "train, test = merge_datasets(train, test, info)\n",
    "\n",
    "print(f\"âœ… Train shape: {train.shape}\")\n",
    "print(f\"âœ… Test shape: {test.shape}\")\n",
    "print(f\"âœ… Building info shape: {info.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering ì‹¤í–‰\n",
    "print(\"ğŸ”§ Feature Engineering ì‹œì‘...\")\n",
    "\n",
    "train = create_datetime(train)\n",
    "test = create_datetime(test)\n",
    "\n",
    "combined_df = pd.concat([train, test], ignore_index=True)\n",
    "print(f\"  âœ“ Combined shape: {combined_df.shape}\")\n",
    "\n",
    "combined_df = add_holiday(combined_df)\n",
    "print(\"  âœ“ Holiday ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "combined_df = remove_outliers(combined_df)\n",
    "print(f\"  âœ“ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ (shape: {combined_df.shape})\")\n",
    "\n",
    "combined_df = add_squared_features(combined_df)\n",
    "print(\"  âœ“ Squared features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_summer_cycle_features(combined_df)\n",
    "print(\"  âœ“ Summer cycle features ì¶”ê°€\")\n",
    "\n",
    "combined_df = create_cyclic_features(combined_df)\n",
    "print(\"  âœ“ Cyclic features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_cdh_feature(combined_df)\n",
    "print(\"  âœ“ CDH ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_cdd_feature(combined_df)\n",
    "print(\"  âœ“ CDD ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_thi_feature(combined_df)\n",
    "print(\"  âœ“ THI ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_wct_feature(combined_df)\n",
    "print(\"  âœ“ WCT ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_temp_features(combined_df)\n",
    "print(\"  âœ“ Temperature features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_humid_features(combined_df)\n",
    "print(\"  âœ“ Humidity features ì¶”ê°€\")\n",
    "\n",
    "combined_df = mean_std_power(combined_df)\n",
    "print(\"  âœ“ Target í†µê³„ features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_weekly_slope(combined_df)\n",
    "print(\"  âœ“ Weekly slope ì¶”ê°€\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… Feature Engineering ì™„ë£Œ! Shape: {combined_df.shape}\")\n",
    "print(f\"   ì´ ì»¬ëŸ¼ ìˆ˜: {len(combined_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test ë¶„ë¦¬ ë° ì „ì²˜ë¦¬\n",
    "split_date = pd.to_datetime('2024-08-25 00:00:00')\n",
    "\n",
    "x_full_train = combined_df[combined_df['date_time'] < split_date].copy()\n",
    "test_data = combined_df[combined_df['date_time'] >= split_date].copy()\n",
    "\n",
    "for c in CAT_COLS:\n",
    "    x_full_train[c] = x_full_train[c].astype('category')\n",
    "    test_data[c] = test_data[c].astype('category')\n",
    "\n",
    "x_full_train = x_full_train.ffill()\n",
    "\n",
    "# ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ drop (ì•ˆì „ì„± ì¶”ê°€)\n",
    "x_full_train.drop(columns=[col for col in DROP_COLS if col in x_full_train.columns], inplace=True)\n",
    "test_data.drop(columns=[col for col in DROP_COLS if col in test_data.columns], inplace=True)\n",
    "\n",
    "print(f\"âœ… Train shape: {x_full_train.shape}\")\n",
    "print(f\"âœ… Test shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Section 3: ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "### 3.1 TabPFN Stacking í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking í•¨ìˆ˜ ì •ì˜\n",
    "def get_stacking_ml_datasets(model, X_train_n, y_train_n, X_test_n, n_folds, seed=42):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n, y_train_n)):\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "        \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n",
    "    return train_fold_pred, test_pred_mean\n",
    "\n",
    "print(\"âœ… Stacking í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì‚¬ìš© ì„¤ì • ë° í™•ì¸\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
    "    print(\"âš ï¸ Colabì—ì„œ ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì¤€ë¹„\n",
    "KI_train = x_full_train.copy()\n",
    "KI_test = test_data.copy()\n",
    "\n",
    "KI_train['ê±´ë¬¼ë²ˆí˜¸'] = KI_train['building_number']\n",
    "KI_test['ê±´ë¬¼ë²ˆí˜¸'] = KI_test['building_number']\n",
    "KI_train['ì¼ì‹œ'] = KI_train['date_time']\n",
    "KI_test['ì¼ì‹œ'] = KI_test['date_time']\n",
    "KI_train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] = KI_train['power_consumption']\n",
    "\n",
    "drop_columns = ['building_type', 'total_area', 'cooling_area', 'date']\n",
    "KI_train = KI_train.drop([col for col in drop_columns if col in KI_train.columns], axis=1)\n",
    "KI_test = KI_test.drop([col for col in drop_columns if col in KI_test.columns], axis=1)\n",
    "\n",
    "train_df = KI_train.drop(['building_number', 'date_time', 'power_consumption'], axis=1)\n",
    "test_drop_cols = ['building_number', 'date_time'] + (['power_consumption'] if 'power_consumption' in KI_test.columns else [])\n",
    "test_df = KI_test.drop(test_drop_cols, axis=1)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"   Train features: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ëª¨ë¸ 1-5: ê±´ë¬¼ë³„ TabPFN (5ê°œ ì‹œë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5ê°œ ì‹œë“œë¡œ ê±´ë¬¼ë³„ TabPFN í•™ìŠµ\n",
    "# ì£¼ì˜: ì´ ë¶€ë¶„ì€ ì‹¤í–‰ ì‹œê°„ì´ ë§¤ìš° ê¹ë‹ˆë‹¤ (GPU: ì•½ 4-5ì‹œê°„, CPU: 15-20ì‹œê°„)\n",
    "\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "CV_FOLDS = 10\n",
    "\n",
    "# ê° ì‹œë“œë³„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "all_seed_predictions = []\n",
    "\n",
    "print(\"ğŸš€ ê±´ë¬¼ë³„ TabPFN í•™ìŠµ ì‹œì‘ (5ê°œ ì‹œë“œ)...\")\n",
    "print(f\"âš ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {'4-5ì‹œê°„ (GPU)' if device == 'cuda' else '15-20ì‹œê°„ (CPU)'}\\n\")\n",
    "\n",
    "for seed_idx, seed in enumerate(SEEDS, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ² Seed {seed_idx}/5: {seed}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    set_seeds(seed)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        tabpfn = TabPFNRegressor(device='cuda', random_state=seed)\n",
    "    else:\n",
    "        tabpfn = TabPFNRegressor(device='cpu', random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    best_ml = [tabpfn]\n",
    "    scaler = StandardScaler()\n",
    "    preds_total = []\n",
    "    \n",
    "    for b_num in tqdm(train_df['ê±´ë¬¼ë²ˆí˜¸'].unique(), desc=f\"Seed {seed}\"):\n",
    "        train_b = train_df[train_df[\"ê±´ë¬¼ë²ˆí˜¸\"] == b_num]\n",
    "        test_b = test_df[test_df[\"ê±´ë¬¼ë²ˆí˜¸\"] == b_num]\n",
    "\n",
    "        X_train = train_b.drop(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'], axis=1)\n",
    "        y_train = train_b['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].values\n",
    "        X_test = test_b.drop(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], axis=1)\n",
    "\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        meta_X_train = []\n",
    "        meta_X_test = []\n",
    "        \n",
    "        for estimator in best_ml:\n",
    "            temp_X_train, temp_X_test = get_stacking_ml_datasets(\n",
    "                estimator, X_train, y_train, X_test, CV_FOLDS, seed=seed\n",
    "            )\n",
    "            meta_X_train.append(temp_X_train)\n",
    "            meta_X_test.append(temp_X_test)\n",
    "            \n",
    "        meta_X_train = np.hstack(meta_X_train)\n",
    "        meta_X_test = np.hstack(meta_X_test)\n",
    "\n",
    "        meta_clf = LinearRegression()\n",
    "        meta_clf.fit(meta_X_train, y_train)\n",
    "        preds_partial = meta_clf.predict(meta_X_test)\n",
    "        \n",
    "        preds_total.append(preds_partial)\n",
    "    \n",
    "    prediction_seed = np.hstack(preds_total)\n",
    "    all_seed_predictions.append(prediction_seed)\n",
    "    \n",
    "    print(f\"âœ… Seed {seed} ì™„ë£Œ! Prediction shape: {prediction_seed.shape}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë“  ì‹œë“œ TabPFN í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"   ì´ {len(all_seed_predictions)}ê°œ ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ëª¨ë¸ 6: AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# AutoGluon ëª¨ë¸ í•™ìŠµ\n# ì£¼ì˜: ì´ ë¶€ë¶„ì€ ì‹¤í–‰ ì‹œê°„ì´ ê¹ë‹ˆë‹¤ (ì•½ 1-2ì‹œê°„)\n\nprint(\"ğŸš€ AutoGluon ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\nprint(\"âš ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 1-2ì‹œê°„\\n\")\n\nset_seeds(42)\n\n# AutoGluonìš© ë°ì´í„° ì¤€ë¹„\nag_train = KI_train.copy()\nag_test = KI_test.copy()\n\n# ì»¬ëŸ¼ ì •ë¦¬ - 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'ë„ dropí•˜ì—¬ í”¼ì²˜ì—ì„œ ì œì™¸\ndrop_cols_ag = ['building_number', 'date_time', 'power_consumption', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']\nag_train_features = ag_train.drop([col for col in drop_cols_ag if col in ag_train.columns], axis=1)\nag_train_features['target'] = ag_train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']\n\ndrop_cols_test = ['building_number', 'date_time', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ']\nif 'power_consumption' in ag_test.columns:\n    drop_cols_test.append('power_consumption')\nif 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in ag_test.columns:\n    drop_cols_test.append('ì „ë ¥ì†Œë¹„ëŸ‰(kWh)')\nag_test_features = ag_test.drop([col for col in drop_cols_test if col in ag_test.columns], axis=1)\n\nprint(f\"í•™ìŠµ ë°ì´í„° shape: {ag_train_features.shape}\")\nprint(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {ag_test_features.shape}\")\n\n# AutoGluon í•™ìŠµ\npredictor = TabularPredictor(\n    label='target',\n    problem_type='regression',\n    eval_metric='mean_absolute_error',\n    path='./ag_models'\n).fit(\n    train_data=ag_train_features,\n    time_limit=7200,  # 2ì‹œê°„\n    presets='best_quality',\n    num_gpus=1 if device == 'cuda' else 0,\n    verbosity=2\n)\n\n# ì˜ˆì¸¡\nag_predictions = predictor.predict(ag_test_features)\n\nprint(f\"\\nâœ… AutoGluon í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"   ì˜ˆì¸¡ shape: {ag_predictions.shape}\")\nprint(f\"\\nğŸ“Š ëª¨ë¸ ë¦¬ë”ë³´ë“œ:\")\nprint(predictor.leaderboard())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ëª¨ë¸ 7-16: ê±´ë¬¼ íƒ€ì…ë³„ TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê±´ë¬¼ íƒ€ì…ë³„ TabPFN í•™ìŠµ\n# ì£¼ì˜: ì´ ë¶€ë¶„ì€ ì‹¤í–‰ ì‹œê°„ì´ ê¹ë‹ˆë‹¤ (GPU: ì•½ 1-2ì‹œê°„)\n\nprint(\"ğŸš€ ê±´ë¬¼ íƒ€ì…ë³„ TabPFN í•™ìŠµ ì‹œì‘...\")\nprint(f\"âš ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {'1-2ì‹œê°„ (GPU)' if device == 'cuda' else '5-8ì‹œê°„ (CPU)'}\\n\")\n\nset_seeds(42)\n\n# ê±´ë¬¼ íƒ€ì… ì •ë³´ ê°€ì ¸ì˜¤ê¸°\nbuilding_types = info[['building_number', 'building_type']]\ntrain_with_type = KI_train.merge(building_types, left_on='ê±´ë¬¼ë²ˆí˜¸', right_on='building_number', how='left')\ntest_with_type = KI_test.merge(building_types, left_on='ê±´ë¬¼ë²ˆí˜¸', right_on='building_number', how='left')\n\n# ê° íƒ€ì…ë³„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\ntype_predictions = {}\n\n# TabPFN ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ\nMAX_SAMPLES = 9000  # TabPFN ì œí•œ (10000) ë³´ë‹¤ ì‘ê²Œ ì„¤ì •\n\nif device == 'cuda':\n    tabpfn_type = TabPFNRegressor(device='cuda', random_state=42)\nelse:\n    tabpfn_type = TabPFNRegressor(device='cpu', random_state=42, n_jobs=-1)\n\n# ê° ê±´ë¬¼ íƒ€ì…ë³„ë¡œ í•™ìŠµ\nunique_types = train_with_type['building_type'].unique()\nprint(f\"ì´ {len(unique_types)}ê°œ ê±´ë¬¼ íƒ€ì…: {list(unique_types)}\\n\")\n\nfor btype in tqdm(unique_types, desc=\"Building Type\"):\n    train_type = train_with_type[train_with_type['building_type'] == btype]\n    test_type = test_with_type[test_with_type['building_type'] == btype]\n    \n    if len(train_type) == 0 or len(test_type) == 0:\n        print(f\"âš ï¸ {btype}: ë°ì´í„° ì—†ìŒ, ê±´ë„ˆëœ€\")\n        continue\n    \n    # í”¼ì²˜ ì¤€ë¹„ - ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ drop\n    drop_train_cols = ['building_number', 'building_type', 'date_time', \n                       'power_consumption', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']\n    drop_train_cols = [col for col in drop_train_cols if col in train_type.columns]\n    X_train = train_type.drop(drop_train_cols, axis=1)\n    y_train = train_type['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].values\n    \n    drop_test_cols = ['building_number', 'building_type', 'date_time', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ']\n    if 'power_consumption' in test_type.columns:\n        drop_test_cols.append('power_consumption')\n    drop_test_cols = [col for col in drop_test_cols if col in test_type.columns]\n    X_test = test_type.drop(drop_test_cols, axis=1)\n    \n    # TabPFN ìƒ˜í”Œ ìˆ˜ ì œí•œ ì²˜ë¦¬\n    if len(X_train) > MAX_SAMPLES:\n        print(f\"  âš ï¸ {btype}: {len(X_train)}ê°œ ìƒ˜í”Œì„ {MAX_SAMPLES}ê°œë¡œ ìƒ˜í”Œë§\")\n        sample_indices = np.random.choice(len(X_train), MAX_SAMPLES, replace=False)\n        X_train_sampled = X_train.iloc[sample_indices]\n        y_train_sampled = y_train[sample_indices]\n    else:\n        X_train_sampled = X_train\n        y_train_sampled = y_train\n    \n    # ìŠ¤ì¼€ì¼ë§\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train_sampled)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Stacking\n    meta_X_train, meta_X_test = get_stacking_ml_datasets(\n        tabpfn_type, X_train_scaled, y_train_sampled, X_test_scaled, CV_FOLDS, seed=42\n    )\n    \n    # Meta ëª¨ë¸ í•™ìŠµ (ì›ë³¸ ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n    meta_clf = LinearRegression()\n    meta_clf.fit(meta_X_train, y_train_sampled)\n    preds_type = meta_clf.predict(meta_X_test)\n    \n    # ê±´ë¬¼ë²ˆí˜¸ì™€ í•¨ê»˜ ì €ì¥\n    type_predictions[btype] = {\n        'predictions': preds_type,\n        'building_numbers': test_type['ê±´ë¬¼ë²ˆí˜¸'].values\n    }\n    \n    print(f\"  âœ“ {btype}: {len(preds_type)}ê°œ ì˜ˆì¸¡ ì™„ë£Œ\")\n    \n    if device == 'cuda':\n        torch.cuda.empty_cache()\n\n# ê±´ë¬¼ íƒ€ì…ë³„ ì˜ˆì¸¡ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•© (ê±´ë¬¼ë²ˆí˜¸ ìˆœì„œì— ë§ì¶°)\ntype_pred_full = np.zeros(len(test_df))\ntest_building_nums = test_df['ê±´ë¬¼ë²ˆí˜¸'].values\n\nfor btype, data in type_predictions.items():\n    for i, bnum in enumerate(data['building_numbers']):\n        idx = np.where(test_building_nums == bnum)[0]\n        if len(idx) > 0:\n            type_pred_full[idx[0]:idx[0]+1] = data['predictions'][i]\n\nprint(f\"\\nâœ… ê±´ë¬¼ íƒ€ì…ë³„ TabPFN í•™ìŠµ ì™„ë£Œ!\")\nprint(f\"   ì˜ˆì¸¡ shape: {type_pred_full.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Section 4: ìµœì¢… ì•™ìƒë¸”\n",
    "\n",
    "1ë“± íŒ€ì˜ ì•™ìƒë¸” ì „ëµ:\n",
    "- 5ê°œ ì‹œë“œ ê±´ë¬¼ë³„ ëª¨ë¸: ê° 20% (ì´ 100%)\n",
    "- AutoGluon: ì¶”ê°€ ê°€ì¤‘ì¹˜\n",
    "- ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸: ì¶”ê°€ ê°€ì¤‘ì¹˜\n",
    "\n",
    "ìµœì  ê°€ì¤‘ì¹˜ëŠ” ì‹¤í—˜ì„ í†µí•´ ì¡°ì • ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ì•™ìƒë¸”\n",
    "print(\"ğŸ¯ ìµœì¢… ì•™ìƒë¸” ì‹œì‘...\\n\")\n",
    "\n",
    "# 1. 5ê°œ ì‹œë“œ ëª¨ë¸ í‰ê·  (ê¸°ë³¸ ì˜ˆì¸¡)\n",
    "seed_ensemble = np.mean(all_seed_predictions, axis=0)\n",
    "print(f\"âœ“ 5ê°œ ì‹œë“œ ëª¨ë¸ í‰ê· : {seed_ensemble.shape}\")\n",
    "\n",
    "# 2. ìµœì  ê°€ì¤‘ì¹˜ ì•™ìƒë¸” (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥)\n",
    "# ê°€ì¤‘ì¹˜: ì‹œë“œ í‰ê·  60%, AutoGluon 25%, íƒ€ì…ë³„ 15%\n",
    "weight_seeds = 0.60\n",
    "weight_ag = 0.25\n",
    "weight_type = 0.15\n",
    "\n",
    "final_prediction = (\n",
    "    weight_seeds * seed_ensemble +\n",
    "    weight_ag * ag_predictions.values +\n",
    "    weight_type * type_pred_full\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì•™ìƒë¸” ì™„ë£Œ!\")\n",
    "print(f\"   ê°€ì¤‘ì¹˜: ì‹œë“œí‰ê· ={weight_seeds}, AutoGluon={weight_ag}, íƒ€ì…ë³„={weight_type}\")\n",
    "print(f\"   ì˜ˆì¸¡ shape: {final_prediction.shape}\")\n",
    "print(f\"   ì˜ˆì¸¡ê°’ ë²”ìœ„: [{final_prediction.min():.2f}, {final_prediction.max():.2f}]\")\n",
    "print(f\"   ì˜ˆì¸¡ê°’ í‰ê· : {final_prediction.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Section 5: ì œì¶œ íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
    "submission['answer'] = np.round(final_prediction, 2)\n",
    "submission.to_csv('advanced_ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"âœ… advanced_ensemble_submission.csv ì €ì¥ ì™„ë£Œ\\n\")\n",
    "print(\"ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼ í†µê³„:\")\n",
    "print(f\"  - ì´ ì˜ˆì¸¡ ê°œìˆ˜: {len(submission)}\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ ë²”ìœ„: [{submission['answer'].min():.2f}, {submission['answer'].max():.2f}]\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ í‰ê· : {submission['answer'].mean():.2f}\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ ì¤‘ì•™ê°’: {submission['answer'].median():.2f}\")\n",
    "print(f\"\\nê²°ì¸¡ì¹˜ í™•ì¸: {submission.isnull().sum().sum()}\")\n",
    "\n",
    "if submission.isnull().sum().sum() == 0:\n",
    "    print(\"\\nâœ… ì œì¶œ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    print(\"\\nì˜ˆì¸¡ê°’ ìƒ˜í”Œ:\")\n",
    "    print(submission.head(10))\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 6: ê°œë³„ ëª¨ë¸ ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë„ ì €ì¥\n",
    "print(\"ğŸ’¾ ê°œë³„ ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì¤‘...\\n\")\n",
    "\n",
    "# 1. 5ê°œ ì‹œë“œ ëª¨ë¸ ê°œë³„ ì €ì¥\n",
    "for idx, seed in enumerate(SEEDS, 1):\n",
    "    submission_seed = submission.copy()\n",
    "    submission_seed['answer'] = np.round(all_seed_predictions[idx-1], 2)\n",
    "    submission_seed.to_csv(f'seed_{seed}_submission.csv', index=False)\n",
    "    print(f\"  âœ“ seed_{seed}_submission.csv\")\n",
    "\n",
    "# 2. ì‹œë“œ í‰ê·  ì €ì¥\n",
    "submission_seeds = submission.copy()\n",
    "submission_seeds['answer'] = np.round(seed_ensemble, 2)\n",
    "submission_seeds.to_csv('seeds_ensemble_submission.csv', index=False)\n",
    "print(f\"  âœ“ seeds_ensemble_submission.csv\")\n",
    "\n",
    "# 3. AutoGluon ì €ì¥\n",
    "submission_ag = submission.copy()\n",
    "submission_ag['answer'] = np.round(ag_predictions.values, 2)\n",
    "submission_ag.to_csv('autogluon_submission.csv', index=False)\n",
    "print(f\"  âœ“ autogluon_submission.csv\")\n",
    "\n",
    "# 4. íƒ€ì…ë³„ ì €ì¥\n",
    "submission_type = submission.copy()\n",
    "submission_type['answer'] = np.round(type_pred_full, 2)\n",
    "submission_type.to_csv('type_ensemble_submission.csv', index=False)\n",
    "print(f\"  âœ“ type_ensemble_submission.csv\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ê°œë³„ ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ì™„ë£Œ!\n",
    "\n",
    "**ìƒì„±ëœ íŒŒì¼:**\n",
    "1. `advanced_ensemble_submission.csv` - ìµœì¢… ì•™ìƒë¸” ê²°ê³¼ (ì œì¶œìš©)\n",
    "2. `seed_XXX_submission.csv` - ê° ì‹œë“œë³„ ê²°ê³¼ (5ê°œ)\n",
    "3. `seeds_ensemble_submission.csv` - 5ê°œ ì‹œë“œ í‰ê· \n",
    "4. `autogluon_submission.csv` - AutoGluon ê²°ê³¼\n",
    "5. `type_ensemble_submission.csv` - ê±´ë¬¼ íƒ€ì…ë³„ ê²°ê³¼\n",
    "\n",
    "**ë‹¤ìŒ ë‹¨ê³„:**\n",
    "1. `advanced_ensemble_submission.csv`ë¥¼ ë¦¬ë”ë³´ë“œì— ì œì¶œ\n",
    "2. ê°œë³„ ëª¨ë¸ ê²°ê³¼ë„ ì œì¶œí•˜ì—¬ ì„±ëŠ¥ ë¹„êµ\n",
    "3. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¡°ì • ì‹¤í—˜ (Section 4ì˜ weight ê°’ ë³€ê²½)\n",
    "4. ì¶”ê°€ Feature Engineering ì‹œë„\n",
    "\n",
    "**ì°¸ê³ :**\n",
    "- ì‹¤í–‰ ì‹œê°„: ì•½ 5-8ì‹œê°„ (GPU), 20-30ì‹œê°„ (CPU)\n",
    "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ 8-16GB\n",
    "- GPU ê¶Œì¥: Colab Pro, Tesla T4 ì´ìƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}