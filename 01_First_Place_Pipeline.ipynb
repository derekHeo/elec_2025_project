{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ† 1ë“± íŒ€ ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "**ì‹¤í–‰ í™˜ê²½**: Google Colab (GPU í•„ìˆ˜)\n",
    "\n",
    "**ì‹¤í–‰ ìˆœì„œ**:\n",
    "1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "2. ë°ì´í„° ì—…ë¡œë“œ (Colabì— data í´ë” ì—…ë¡œë“œ)\n",
    "3. ì „ì²˜ë¦¬\n",
    "4. ëª¨ë¸ í•™ìŠµ (ê° ì„¹ì…˜ ì„ íƒ ì‹¤í–‰ ê°€ëŠ¥)\n",
    "5. ìµœì¢… ì•™ìƒë¸”\n",
    "\n",
    "**ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„**: ì•½ 3-5ì‹œê°„ (GPU ì‚¬ìš© ì‹œ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Section 0: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q tabpfn==2.1.2\n",
    "!pip install -q tabpfn-extensions==0.1.3\n",
    "!pip install -q autogluon==1.2\n",
    "!pip install -q scikit-learn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "print(f\"âœ… ëœë¤ ì‹œë“œ ì„¤ì •: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (Colabì—ì„œëŠ” data í´ë”ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”)\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# ë°ì´í„° í´ë” êµ¬ì¡° í™•ì¸\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"âœ… ë°ì´í„° í´ë” ë°œê²¬\")\n",
    "    print(\"\\ní´ë” ë‚´ìš©:\")\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. data í´ë”ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.\")\n",
    "    print(\"\\ní•„ìš”í•œ íŒŒì¼:\")\n",
    "    print(\"  - train.csv\")\n",
    "    print(\"  - test.csv\")\n",
    "    print(\"  - building_info.csv\")\n",
    "    print(\"  - sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Section 1: ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒìˆ˜ ì •ì˜\n",
    "WEEK_H = 168  # 1ì£¼ì¼ = 168ì‹œê°„\n",
    "EPS = 1e-3\n",
    "\n",
    "# ì»¬ëŸ¼ëª… ë§¤í•‘\n",
    "TRAIN_COL_RENAMES = {\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ì¼ì‹œ': 'date_time',\n",
    "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
    "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
    "    'í’ì†(m/s)': 'windspeed',\n",
    "    'ìŠµë„(%)': 'humidity',\n",
    "    'ì¼ì¡°(hr)': 'sunshine',\n",
    "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
    "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
    "}\n",
    "TEST_COL_RENAMES = TRAIN_COL_RENAMES.copy()\n",
    "\n",
    "BUILDING_INFO_RENAMES = {\n",
    "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
    "    'ê±´ë¬¼ìœ í˜•': 'building_type',\n",
    "    'ì—°ë©´ì (m2)': 'total_area',\n",
    "    'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',\n",
    "    'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'solar_power_capacity',\n",
    "    'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',\n",
    "    'PCSìš©ëŸ‰(kW)': 'pcs_capacity'\n",
    "}\n",
    "\n",
    "TYPE_TRANSLATION = {\n",
    "    'ê±´ë¬¼ê¸°íƒ€': 'Other Buildings',\n",
    "    'ê³µê³µ': 'Public',\n",
    "    'í•™êµ': 'School',\n",
    "    'ë°±í™”ì ': 'Department Store',\n",
    "    'ë³‘ì›': 'Hospital',\n",
    "    'ìƒìš©': 'Commercial',\n",
    "    'ì•„íŒŒíŠ¸': 'Apartment',\n",
    "    'ì—°êµ¬ì†Œ': 'Research Institute',\n",
    "    'í˜¸í…”': 'Hotel',\n",
    "    'IDC(ì „í™”êµ­)': 'IDC'\n",
    "}\n",
    "\n",
    "KR_HOLIDAYS_2024 = {\"2024-06-06\", \"2024-08-15\"}\n",
    "\n",
    "DROP_COLS = ['sunshine', 'solar_radiation', 'solar_power_capacity', 'ess_capacity', 'pcs_capacity', \n",
    "             'hour', 'day_of_week', 'day_of_year']\n",
    "CAT_COLS = ['building_type', 'building_number']\n",
    "\n",
    "print(\"âœ… ìƒìˆ˜ ë° ë§¤í•‘ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_raw(data_dir: str = \"./data\"):\n",
    "    train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    info = pd.read_csv(os.path.join(data_dir, 'building_info.csv'))\n",
    "    return train, test, info\n",
    "\n",
    "def rename_columns(df: pd.DataFrame, mapping: dict):\n",
    "    df = df.rename(columns=mapping)\n",
    "    if 'num_date_time' in df.columns:\n",
    "        df = df.drop('num_date_time', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_building_info(info: pd.DataFrame) -> pd.DataFrame:\n",
    "    info = info.rename(columns=BUILDING_INFO_RENAMES)\n",
    "    info['building_type'] = info['building_type'].replace(TYPE_TRANSLATION)\n",
    "    return info\n",
    "\n",
    "def merge_datasets(train: pd.DataFrame, test: pd.DataFrame, info: pd.DataFrame):\n",
    "    train = train.merge(info, on='building_number', how='left')\n",
    "    test = test.merge(info, on='building_number', how='left')\n",
    "    return train, test\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering í•¨ìˆ˜ë“¤\n",
    "def create_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d %H')\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['day_of_year'] = df['date_time'].dt.dayofyear\n",
    "    return df\n",
    "\n",
    "def add_summer_cycle_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    start_date = datetime.strptime(\"2024-05-20 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2024-09-08 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    period_seconds = (end_date - start_date).total_seconds()\n",
    "    \n",
    "    def summer_cos(date):\n",
    "        return np.cos(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "    \n",
    "    def summer_sin(date):\n",
    "        return np.sin(2 * np.pi * (date - start_date).total_seconds() / period_seconds)\n",
    "        \n",
    "    df_copy['summer_cos'] = df_copy['date_time'].apply(summer_cos)\n",
    "    df_copy['summer_sin'] = df_copy['date_time'].apply(summer_sin)\n",
    "    return df_copy\n",
    "\n",
    "def add_squared_features(df: pd.DataFrame, target_cols: List[str] = ['temperature', 'humidity']) -> pd.DataFrame:\n",
    "    df_copy = df.copy()\n",
    "    for col in target_cols:\n",
    "        df_copy[f'{col}_squared'] = df_copy[col] ** 2\n",
    "    return df_copy\n",
    "\n",
    "def create_cyclic_features(df):\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['sin_doy'] = np.sin(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    df['cos_doy'] = np.cos(2 * np.pi * (df['day_of_year'] - 1) / 365)\n",
    "    return df\n",
    "\n",
    "def cooling_degree_hour(temperature, window=12, base_temp=26):\n",
    "    cdhs = []\n",
    "    temps = temperature.values\n",
    "    for i in range(len(temps)):\n",
    "        if i < window:\n",
    "            cdh = np.sum(np.maximum(temps[:i+1] - base_temp, 0))\n",
    "        else:\n",
    "            cdh = np.sum(np.maximum(temps[i-window+1:i+1] - base_temp, 0))\n",
    "        cdhs.append(cdh)\n",
    "    return cdhs\n",
    "\n",
    "def add_cdh_feature(df: pd.DataFrame, window: int = 12, base_temp: float = 26.0) -> pd.DataFrame:\n",
    "    cdhs_all = []\n",
    "    for b in df['building_number'].unique():\n",
    "        temps = df.loc[df['building_number'] == b, 'temperature']\n",
    "        cdhs_all.extend(cooling_degree_hour(temps, window=window, base_temp=base_temp))\n",
    "    df['CDH'] = cdhs_all\n",
    "    return df\n",
    "\n",
    "def add_cdd_feature(df: pd.DataFrame, base_temp: float = 18.0, window: int = 24) -> pd.DataFrame:\n",
    "    df['excess'] = (df['temperature'] - base_temp).clip(lower=0)\n",
    "    df['CDD'] = df.groupby('building_number')['excess'].transform(\n",
    "        lambda s: s.rolling(window, min_periods=1).sum()\n",
    "    )\n",
    "    df.drop(columns=['excess'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_thi_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['THI'] = (9/5 * df['temperature'] \n",
    "                 - 0.55 * (1 - df['humidity']/100) \n",
    "                 * (9/5 * df['temperature'] - 26) \n",
    "                 + 32)\n",
    "    return df\n",
    "\n",
    "def add_wct_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    v16 = df['windspeed'] ** 0.16\n",
    "    df['WCT'] = (13.12 \n",
    "                 + 0.6215 * df['temperature'] \n",
    "                 - 11.37 * v16 \n",
    "                 + 0.3965 * v16 * df['temperature'])\n",
    "    return df\n",
    "\n",
    "def add_temp_features(data):\n",
    "    avg_temp = pd.pivot_table(\n",
    "        data[data['hour'] % 3 == 0],\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='mean'\n",
    "    ).reset_index().rename(columns={'temperature': 'avg_temp'})\n",
    "    data = pd.merge(data, avg_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    max_temp = pd.pivot_table(\n",
    "        data,\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='max'\n",
    "    ).reset_index().rename(columns={'temperature': 'max_temp'})\n",
    "    data = pd.merge(data, max_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    min_temp = pd.pivot_table(\n",
    "        data,\n",
    "        values='temperature',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='min'\n",
    "    ).reset_index().rename(columns={'temperature': 'min_temp'})\n",
    "    data = pd.merge(data, min_temp, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    data['temp_diff'] = data['max_temp'] - data['min_temp']\n",
    "    return data\n",
    "\n",
    "def add_humid_features(data):\n",
    "    avg_humid = pd.pivot_table(\n",
    "        data[data['hour'] % 3 == 0],\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='mean'\n",
    "    ).reset_index().rename(columns={'humidity': 'avg_humid'})\n",
    "    data = pd.merge(data, avg_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    max_humid = pd.pivot_table(\n",
    "        data,\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='max'\n",
    "    ).reset_index().rename(columns={'humidity': 'max_humid'})\n",
    "    data = pd.merge(data, max_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    min_humid = pd.pivot_table(\n",
    "        data,\n",
    "        values='humidity',\n",
    "        index=['building_number', 'day', 'month'],\n",
    "        aggfunc='min'\n",
    "    ).reset_index().rename(columns={'humidity': 'min_humid'})\n",
    "    data = pd.merge(data, min_humid, on=['building_number', 'day', 'month'], how='left')\n",
    "\n",
    "    data['humid_diff'] = data['max_humid'] - data['min_humid']\n",
    "    return data\n",
    "\n",
    "def _prep(df, time_col, group_col):\n",
    "    return df.sort_values([group_col, time_col])\n",
    "\n",
    "def add_weekly_slope(df: pd.DataFrame, time_col: str = 'date_time',\n",
    "                     group_col: str = 'building_number',\n",
    "                     power_col: str = 'power_consumption',\n",
    "                     lookback: int = 6) -> pd.DataFrame:\n",
    "    df = _prep(df, time_col, group_col)\n",
    "\n",
    "    def _beta(x: pd.Series) -> float:\n",
    "        if x.isna().any(): \n",
    "            return np.nan\n",
    "        idx = np.arange(len(x))\n",
    "        num = idx.dot(x) * len(x) - idx.sum() * x.sum()\n",
    "        den = len(x) * (idx**2).sum() - idx.sum()**2\n",
    "        return num / den if den else 0.0\n",
    "\n",
    "    pw_seq = df.groupby(group_col)[power_col].shift(WEEK_H)\n",
    "    col = f'power_week_slope{lookback}h'\n",
    "    df[col] = pw_seq.groupby(df[group_col]).transform(\n",
    "        lambda s: s.rolling(lookback).apply(_beta, raw=False)\n",
    "    ).fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Feature Engineering í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íœ´ì¼ ì„¤ì • í•¨ìˆ˜ (ê±´ë¬¼ë³„ ì„¸ë°€í•œ ì„¤ì •)\n",
    "def _ensure_dt(df):\n",
    "    if not np.issubdtype(df[\"date_time\"].dtype, np.datetime64):\n",
    "        df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    return df\n",
    "\n",
    "def _nth_weekday_in_month(series_dt, weekday_target):\n",
    "    first_of_month = series_dt.values.astype(\"datetime64[M]\").astype(\"datetime64[ns]\")\n",
    "    first_weekday = pd.to_datetime(first_of_month).weekday\n",
    "    weekday = series_dt.dt.weekday.values\n",
    "    day = series_dt.dt.day.values\n",
    "    first_occ_day = 1 + ((weekday_target - first_weekday) % 7)\n",
    "    nth = ((day - first_occ_day) // 7) + 1\n",
    "    nth = np.where(day >= first_occ_day, nth, 0)\n",
    "    return nth\n",
    "\n",
    "def add_holiday(df: pd.DataFrame, kr_holidays: set[str] = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "    if kr_holidays is None:\n",
    "        kr_holidays = KR_HOLIDAYS_2024\n",
    "\n",
    "    df[\"weekday\"] = df[\"date_time\"].dt.weekday\n",
    "    df[\"date\"] = df[\"date_time\"].dt.date\n",
    "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "    df[\"holiday\"] = 0\n",
    "\n",
    "    is_kr = df[\"date\"].astype(str).isin(kr_holidays).values\n",
    "    bt = df[\"building_type\"]\n",
    "\n",
    "    # Apartment: í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Apartment\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # Hospital: ì£¼ë§ or ê³µíœ´ì¼ íœ´ì‹\n",
    "    mm = bt == \"Hospital\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # Public: ê¸°ë³¸ ì£¼ë§ or ê³µíœ´ì¼, ë‹¨ 33/92ëŠ” í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Public\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "        mm_always_open = df[\"building_number\"].isin([33, 92])\n",
    "        df.loc[mm_always_open, \"holiday\"] = 0\n",
    "\n",
    "    # Hotel: í•­ìƒ ì˜ì—…\n",
    "    mm = bt == \"Hotel\"\n",
    "    df.loc[mm, \"holiday\"] = 0\n",
    "\n",
    "    # School: ì£¼ë§ or ê³µíœ´ì¼ íœ´ì‹\n",
    "    mm = bt == \"School\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "\n",
    "    # IDC: ê°œë³„ ê·œì¹™\n",
    "    mm_idc = bt == \"IDC\"\n",
    "    if mm_idc.any():\n",
    "        ids = [36, 43, 52]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "        mmx = df[\"building_number\"].eq(64)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        mmx = df[\"building_number\"].eq(67)\n",
    "        if mmx.any():\n",
    "            df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "            df.loc[mmx & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "\n",
    "    # Commercial: ê°œë³„ ê·œì¹™\n",
    "    mm = bt == \"Commercial\"\n",
    "    if mm.any():\n",
    "        mmx = df[\"building_number\"].eq(2)\n",
    "        df.loc[mmx, \"holiday\"] = df.loc[mmx, \"is_weekend\"].astype(int)\n",
    "        ids = [6, 16, 20, 51, 86]\n",
    "        mmx = df[\"building_number\"].isin(ids)\n",
    "        df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "\n",
    "    # Other Buildings\n",
    "    mmx = df[\"building_number\"].eq(26)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].isin([0, 1]).astype(int)\n",
    "    mmx = df[\"building_number\"].eq(82)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(0).astype(int)\n",
    "    mmx = df[\"building_number\"].isin([47, 69])\n",
    "    df.loc[mmx, \"holiday\"] = (df.loc[mmx, \"is_weekend\"].values | is_kr[mmx]).astype(int)\n",
    "    mmx = df[\"building_number\"].eq(97)\n",
    "    df.loc[mmx, \"holiday\"] = df.loc[mmx, \"weekday\"].eq(5).astype(int)\n",
    "\n",
    "    # Department Store: ê°œë³„ íœ´ì¼ ê·œì¹™\n",
    "    mm = bt == \"Department Store\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = 0\n",
    "        nth_sun = _nth_weekday_in_month(df[\"date_time\"], 6)\n",
    "        nth_mon = _nth_weekday_in_month(df[\"date_time\"], 0)\n",
    "\n",
    "        def mark_nth_weekday(building, weekday, nth_set):\n",
    "            if weekday == 6:\n",
    "                nth = nth_sun\n",
    "            elif weekday == 0:\n",
    "                nth = nth_mon\n",
    "            else:\n",
    "                nth = _nth_weekday_in_month(df[\"date_time\"], weekday)\n",
    "            sel = df[\"building_number\"].eq(building) & df[\"weekday\"].eq(weekday) & pd.Series(nth).isin(list(nth_set)).values\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        df.loc[df[\"building_number\"].eq(18) & df[\"weekday\"].eq(6), \"holiday\"] = 1\n",
    "\n",
    "        special = {\n",
    "            19: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            45: [\"2024-06-10\", \"2024-07-08\", \"2024-08-19\"],\n",
    "            54: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            74: [\"2024-06-17\", \"2024-07-01\"],\n",
    "            79: [\"2024-06-17\", \"2024-07-01\", \"2024-08-19\"],\n",
    "            95: [\"2024-07-08\", \"2024-08-05\"],\n",
    "            29: [\"2024-06-10\", \"2024-07-10\", \"2024-08-10\"],\n",
    "        }\n",
    "        for b, dates in special.items():\n",
    "            sel = df[\"building_number\"].eq(b) & df[\"date\"].astype(str).isin(dates)\n",
    "            df.loc[sel, \"holiday\"] = 1\n",
    "\n",
    "        mark_nth_weekday(27, 6, {2, 4})\n",
    "        mark_nth_weekday(29, 6, {4})\n",
    "        mark_nth_weekday(32, 0, {2, 4})\n",
    "        for b in [40, 59, 63]:\n",
    "            mark_nth_weekday(b, 6, {2, 4})\n",
    "\n",
    "    # Research Institute\n",
    "    mm = bt == \"Research Institute\"\n",
    "    if mm.any():\n",
    "        df.loc[mm, \"holiday\"] = (df.loc[mm, \"is_weekend\"].values | is_kr[mm]).astype(int)\n",
    "        \n",
    "        nth_fri = _nth_weekday_in_month(df[\"date_time\"], 4)\n",
    "        sel_23 = df[\"building_number\"].eq(23) & mm\n",
    "        df.loc[sel_23 & df[\"weekday\"].eq(4) & pd.Series(nth_fri).eq(3).values, \"holiday\"] = 1\n",
    "        extra_23 = {\"2024-06-07\", \"2024-08-16\"}\n",
    "        df.loc[sel_23 & df[\"date\"].astype(str).isin(extra_23), \"holiday\"] = 1\n",
    "\n",
    "        sel_49 = df[\"building_number\"].eq(49) & mm\n",
    "        df.loc[sel_49 & df[\"date\"].astype(str).eq(\"2024-08-22\"), \"holiday\"] = 1\n",
    "\n",
    "        sel_53 = df[\"building_number\"].eq(53) & mm\n",
    "        extra_53 = {\"2024-06-15\", \"2024-06-16\"}\n",
    "        df.loc[sel_53 & df[\"date\"].astype(str).isin(extra_53), \"holiday\"] = 1\n",
    "\n",
    "        sel_94 = df[\"building_number\"].eq(94) & mm\n",
    "        extra_94 = {\"2024-06-07\", \"2024-08-16\"}\n",
    "        df.loc[sel_94 & df[\"date\"].astype(str).isin(extra_94), \"holiday\"] = 1\n",
    "\n",
    "    df.loc[(df[\"building_number\"].eq(67)) & (df[\"date\"].astype(str) == \"2024-08-15\"), \"holiday\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"âœ… íœ´ì¼ ì„¤ì • í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ìƒì¹˜ ì œê±° ë° íƒ€ê²Ÿ í†µê³„ í•¨ìˆ˜\n",
    "def remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    _ensure_dt(df)\n",
    "\n",
    "    rules_lt = [\n",
    "        (25, 0, \"eq\"), (70, 200, \"lt\"),\n",
    "        (44, 800, \"lt\"), (90, 800, \"lt\"), (42, 2000, \"lt\"), (17, 1000, \"lt\"),\n",
    "        (68, 600, \"lt\"), (72, 600, \"lt\"), (80, 600, \"lt\"), (92, 200, \"lt\"),\n",
    "        (98, 500, \"lt\"),\n",
    "        (97, 500, \"lt\"), (78, 400, \"lt\"), (26, 300, \"lt\"), (7, 2000, \"lt\"),\n",
    "        (76, 2000, \"lt\"), (41, 2200, \"lt\"), (20, 1600, \"lt\"),\n",
    "        (5, 2000, \"lt\"), (8, 250, \"lt\"), (12, 3500, \"lt\"),\n",
    "        (67, 7333, \"lt\"), (81, 800, \"lt\"), (52, 2000, \"lt\"), (43, 6000, \"lt\"), (30, 8000, \"lt\"),\n",
    "    ]\n",
    "\n",
    "    mask_ok = pd.Series(True, index=df.index)\n",
    "    pc = df[\"power_consumption\"]\n",
    "    bnum = df[\"building_number\"]\n",
    "\n",
    "    for bn, th, op in rules_lt:\n",
    "        if op == \"lt\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc < th))\n",
    "        elif op == \"eq\":\n",
    "            mask_ok &= ~((bnum.eq(bn)) & (pc == th))\n",
    "\n",
    "    mask_ok &= ~((bnum.eq(10)) & (df[\"date_time\"].between(pd.Timestamp(\"2024-07-05\"), pd.Timestamp(\"2024-08-22\"))))\n",
    "    mask_ok &= ~((bnum.eq(57)) & (df[\"date_time\"] < pd.Timestamp(\"2024-06-07\")))\n",
    "    mask_ok &= ~((bnum.eq(94)) & (df[\"date_time\"].between(pd.Timestamp(\"2024-07-27 09:00\"), pd.Timestamp(\"2024-08-04 23:00\"))))\n",
    "    mask_ok &= ~((bnum.eq(53)) & (df[\"date_time\"].dt.normalize().isin([pd.Timestamp(\"2024-06-15\"), pd.Timestamp(\"2024-06-16\")])))\n",
    "    mask_ok &= ~((bnum.eq(53)) & (df[\"date_time\"] >= pd.Timestamp(\"2024-08-17\")) & (pc <= 1000))\n",
    "\n",
    "    return df.loc[mask_ok].reset_index(drop=True)\n",
    "\n",
    "def mean_std_power(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    is_train = df['power_consumption'].notna()\n",
    "\n",
    "    dt = pd.to_datetime(df['date_time'])\n",
    "    if 'date' not in df.columns:\n",
    "        df['date'] = dt.dt.date\n",
    "    if 'hour' not in df.columns:\n",
    "        df['hour'] = dt.dt.hour\n",
    "    if 'day_of_week' not in df.columns:\n",
    "        df['day_of_week'] = dt.dt.weekday\n",
    "    if 'month' not in df.columns:\n",
    "        df['month'] = dt.dt.month\n",
    "\n",
    "    df['holiday'] = df['holiday'].fillna(0).astype(int)\n",
    "\n",
    "    base_ratio = np.array([1.0] * 7)\n",
    "    ratio_all = base_ratio - 0\n",
    "    df.loc[is_train, 'power_consumption'] = df.loc[is_train].apply(\n",
    "        lambda r: r['power_consumption'] * ratio_all[int(r['day_of_week'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    train_df = df[is_train].copy()\n",
    "\n",
    "    PUBLIC_HOLS = {\"2024-06-06\", \"2024-08-15\"}\n",
    "    train_df_dow = train_df[~train_df['date'].astype(str).isin(PUBLIC_HOLS)].copy()\n",
    "\n",
    "    dow_hour_mean = train_df_dow.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].mean().reset_index(name='dow_hour_mean')\n",
    "    dow_hour_std = train_df_dow.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].std().reset_index(name='dow_hour_std')\n",
    "    df = df.merge(dow_hour_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "    df = df.merge(dow_hour_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "    hol_mean = train_df.groupby(['building_number', 'hour', 'holiday'])['power_consumption'].mean().reset_index(name='holiday_mean')\n",
    "    hol_std = train_df.groupby(['building_number', 'hour', 'holiday'])['power_consumption'].std().reset_index(name='holiday_std')\n",
    "    df = df.merge(hol_mean, on=['building_number', 'hour', 'holiday'], how='left')\n",
    "    df = df.merge(hol_std, on=['building_number', 'hour', 'holiday'], how='left')\n",
    "\n",
    "    hr_mean = train_df.groupby(['building_number', 'hour'])['power_consumption'].mean().reset_index(name='hour_mean')\n",
    "    hr_std = train_df.groupby(['building_number', 'hour'])['power_consumption'].std().reset_index(name='hour_std')\n",
    "    df = df.merge(hr_mean, on=['building_number', 'hour'], how='left')\n",
    "    df = df.merge(hr_std, on=['building_number', 'hour'], how='left')\n",
    "\n",
    "    mh_mean = train_df.groupby(['building_number', 'month', 'hour'])['power_consumption'].mean().reset_index(name='month_hour_mean')\n",
    "    mh_std = train_df.groupby(['building_number', 'month', 'hour'])['power_consumption'].std().reset_index(name='month_hour_std')\n",
    "    df = df.merge(mh_mean, on=['building_number', 'month', 'hour'], how='left')\n",
    "    df = df.merge(mh_std, on=['building_number', 'month', 'hour'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"âœ… ì´ìƒì¹˜ ì œê±° ë° íƒ€ê²Ÿ í†µê³„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Section 2: ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "print(\"ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "train, test, info = load_raw(DATA_DIR)\n",
    "train = rename_columns(train, TRAIN_COL_RENAMES)\n",
    "test = rename_columns(test, TEST_COL_RENAMES)\n",
    "info = preprocess_building_info(info)\n",
    "train, test = merge_datasets(train, test, info)\n",
    "\n",
    "print(f\"âœ… Train shape: {train.shape}\")\n",
    "print(f\"âœ… Test shape: {test.shape}\")\n",
    "print(f\"âœ… Building info shape: {info.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering ì‹¤í–‰\n",
    "print(\"ğŸ”§ Feature Engineering ì‹œì‘...\")\n",
    "\n",
    "train = create_datetime(train)\n",
    "test = create_datetime(test)\n",
    "\n",
    "combined_df = pd.concat([train, test], ignore_index=True)\n",
    "print(f\"  âœ“ Combined shape: {combined_df.shape}\")\n",
    "\n",
    "combined_df = add_holiday(combined_df)\n",
    "print(\"  âœ“ Holiday ì„¤ì • ì™„ë£Œ\")\n",
    "\n",
    "combined_df = remove_outliers(combined_df)\n",
    "print(f\"  âœ“ ì´ìƒì¹˜ ì œê±° ì™„ë£Œ (shape: {combined_df.shape})\")\n",
    "\n",
    "combined_df = add_squared_features(combined_df)\n",
    "print(\"  âœ“ Squared features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_summer_cycle_features(combined_df)\n",
    "print(\"  âœ“ Summer cycle features ì¶”ê°€\")\n",
    "\n",
    "combined_df = create_cyclic_features(combined_df)\n",
    "print(\"  âœ“ Cyclic features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_cdh_feature(combined_df)\n",
    "print(\"  âœ“ CDH ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_cdd_feature(combined_df)\n",
    "print(\"  âœ“ CDD ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_thi_feature(combined_df)\n",
    "print(\"  âœ“ THI ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_wct_feature(combined_df)\n",
    "print(\"  âœ“ WCT ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_temp_features(combined_df)\n",
    "print(\"  âœ“ Temperature features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_humid_features(combined_df)\n",
    "print(\"  âœ“ Humidity features ì¶”ê°€\")\n",
    "\n",
    "combined_df = mean_std_power(combined_df)\n",
    "print(\"  âœ“ Target í†µê³„ features ì¶”ê°€\")\n",
    "\n",
    "combined_df = add_weekly_slope(combined_df)\n",
    "print(\"  âœ“ Weekly slope ì¶”ê°€\")\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… Feature Engineering ì™„ë£Œ! Shape: {combined_df.shape}\")\n",
    "print(f\"   ì´ ì»¬ëŸ¼ ìˆ˜: {len(combined_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test ë¶„ë¦¬ ë° ì „ì²˜ë¦¬\n",
    "split_date = pd.to_datetime('2024-08-25 00:00:00')\n",
    "\n",
    "x_full_train = combined_df[combined_df['date_time'] < split_date].copy()\n",
    "test = combined_df[combined_df['date_time'] >= split_date].copy()\n",
    "\n",
    "for c in CAT_COLS:\n",
    "    x_full_train[c] = x_full_train[c].astype('category')\n",
    "    test[c] = test[c].astype('category')\n",
    "\n",
    "x_full_train = x_full_train.ffill()\n",
    "\n",
    "x_full_train.drop(columns=DROP_COLS, inplace=True)\n",
    "test.drop(columns=DROP_COLS, inplace=True)\n",
    "\n",
    "print(f\"âœ… Train shape: {x_full_train.shape}\")\n",
    "print(f\"âœ… Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Section 3: TabPFN Stacking ëª¨ë¸ (ê±´ë¬¼ë³„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking í•¨ìˆ˜ ì •ì˜\n",
    "def get_stacking_ml_datasets(model, X_train_n, y_train_n, X_test_n, n_folds, seed=42):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n, y_train_n)):\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "        \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n",
    "    return train_fold_pred, test_pred_mean\n",
    "\n",
    "print(\"âœ… Stacking í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì¤€ë¹„\n",
    "KI_train = x_full_train.copy()\n",
    "KI_test = test.copy()\n",
    "\n",
    "KI_train['ê±´ë¬¼ë²ˆí˜¸'] = KI_train['building_number']\n",
    "KI_test['ê±´ë¬¼ë²ˆí˜¸'] = KI_test['building_number']\n",
    "KI_train['ì¼ì‹œ'] = KI_train['date_time']\n",
    "KI_test['ì¼ì‹œ'] = KI_test['date_time']\n",
    "KI_train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] = KI_train['power_consumption']\n",
    "\n",
    "drop_columns = ['building_type', 'total_area', 'cooling_area', 'date']\n",
    "KI_train = KI_train.drop(drop_columns, axis=1)\n",
    "KI_test = KI_test.drop(drop_columns, axis=1)\n",
    "\n",
    "train_df = KI_train.drop(['building_number', 'date_time', 'power_consumption'], axis=1)\n",
    "test_df = KI_test.drop(['building_number', 'date_time', 'power_consumption'], axis=1)\n",
    "\n",
    "print(f\"âœ… Stackingìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"   Train features: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabPFN Stacking ëª¨ë¸ í•™ìŠµ (ê±´ë¬¼ë³„)\n",
    "# ì£¼ì˜: ì´ ë¶€ë¶„ì€ ì‹¤í–‰ ì‹œê°„ì´ ë§¤ìš° ê¹ë‹ˆë‹¤ (ì•½ 1-2ì‹œê°„)\n",
    "\n",
    "print(\"ğŸš€ TabPFN Stacking í•™ìŠµ ì‹œì‘...\")\n",
    "print(\"âš ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 45-60ë¶„ (GPU ì‚¬ìš© ì‹œ)\")\n",
    "\n",
    "CV_FOLDS = 10\n",
    "scaler = StandardScaler()\n",
    "tabpfn = TabPFNRegressor(random_state=SEED, n_jobs=-1)\n",
    "best_ml = [tabpfn]\n",
    "\n",
    "preds_total = []\n",
    "\n",
    "for b_num in tqdm(train_df['ê±´ë¬¼ë²ˆí˜¸'].unique(), desc=\"Building\"):\n",
    "    train_b = train_df[train_df[\"ê±´ë¬¼ë²ˆí˜¸\"] == b_num]\n",
    "    test_b = test_df[test_df[\"ê±´ë¬¼ë²ˆí˜¸\"] == b_num]\n",
    "\n",
    "    X_train = train_b.drop(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'], axis=1)\n",
    "    y_train = train_b['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].values\n",
    "    X_test = test_b.drop(['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ'], axis=1)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    meta_X_train = []\n",
    "    meta_X_test = []\n",
    "    \n",
    "    for estimator in best_ml:\n",
    "        temp_X_train, temp_X_test = get_stacking_ml_datasets(\n",
    "            estimator, X_train, y_train, X_test, CV_FOLDS\n",
    "        )\n",
    "        meta_X_train.append(temp_X_train)\n",
    "        meta_X_test.append(temp_X_test)\n",
    "        \n",
    "    meta_X_train = np.hstack(meta_X_train)\n",
    "    meta_X_test = np.hstack(meta_X_test)\n",
    "\n",
    "    meta_clf = LinearRegression()\n",
    "    meta_clf.fit(meta_X_train, y_train)\n",
    "    preds_partial = meta_clf.predict(meta_X_test)\n",
    "    \n",
    "    preds_total.append(preds_partial)\n",
    "\n",
    "prediction_stacking = np.hstack(preds_total)\n",
    "\n",
    "print(f\"\\nâœ… TabPFN Stacking ì™„ë£Œ! Prediction shape: {prediction_stacking.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking ê²°ê³¼ ì €ì¥\n",
    "submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n",
    "submission['answer'] = np.round(prediction_stacking, 2)\n",
    "submission.to_csv('stacking_tabpfn.csv', index=False)\n",
    "\n",
    "print(\"âœ… stacking_tabpfn.csv ì €ì¥ ì™„ë£Œ\")\n",
    "print(\"\\nì˜ˆì¸¡ê°’ ìƒ˜í”Œ:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Section 4: ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "\n",
    "**ì°¸ê³ **: 1ë“± íŒ€ì€ ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”í–ˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” TabPFN Stacking ê²°ê³¼ë§Œ ì œì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì „ì²´ ì•™ìƒë¸”ì„ ì¬í˜„í•˜ë ¤ë©´:\n",
    "1. AutoGluon ëª¨ë¸ ì¶”ê°€\n",
    "2. ê±´ë¬¼ íƒ€ì…ë³„ TabPFN ëª¨ë¸ ì¶”ê°€\n",
    "3. ì—¬ëŸ¬ ì‹œë“œë¡œ ì‹¤í–‰ í›„ ì•™ìƒë¸”\n",
    "\n",
    "ì´ ì‘ì—…ë“¤ì€ `02_Advanced_Ensemble.ipynb`ì—ì„œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ì œì¶œ íŒŒì¼ í™•ì¸\n",
    "final_submission = pd.read_csv('stacking_tabpfn.csv')\n",
    "\n",
    "print(\"ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼ í†µê³„:\")\n",
    "print(f\"  - ì´ ì˜ˆì¸¡ ê°œìˆ˜: {len(final_submission)}\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ ë²”ìœ„: [{final_submission['answer'].min():.2f}, {final_submission['answer'].max():.2f}]\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ í‰ê· : {final_submission['answer'].mean():.2f}\")\n",
    "print(f\"  - ì˜ˆì¸¡ê°’ ì¤‘ì•™ê°’: {final_submission['answer'].median():.2f}\")\n",
    "print(f\"\\nê²°ì¸¡ì¹˜ í™•ì¸: {final_submission.isnull().sum().sum()}\")\n",
    "\n",
    "if final_submission.isnull().sum().sum() == 0:\n",
    "    print(\"\\nâœ… ì œì¶œ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **ì„±ëŠ¥ í™•ì¸**: `stacking_tabpfn.csv`ë¥¼ ë¦¬ë”ë³´ë“œì— ì œì¶œ\n",
    "2. **ì¶”ê°€ ëª¨ë¸**: AutoGluon, ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ ë“± ì¶”ê°€ ì‹¤í—˜\n",
    "3. **ì•™ìƒë¸” ìµœì í™”**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•© ë° ê°€ì¤‘ì¹˜ ìµœì í™”\n",
    "4. **Feature Engineering**: ìƒˆë¡œìš´ í”¼ì²˜ ì¶”ê°€ ì‹¤í—˜\n",
    "\n",
    "**ì°¸ê³ **: \n",
    "- 1ë“± íŒ€ì€ 5ê°œ ì‹œë“œì˜ ê±´ë¬¼ë³„ ëª¨ë¸ + AutoGluon + íƒ€ì…ë³„ ëª¨ë¸ì„ ì•™ìƒë¸”\n",
    "- ì „ì²´ ì¬í˜„ì„ ìœ„í•´ì„œëŠ” ì•½ 3-5ì‹œê°„ì˜ í•™ìŠµ ì‹œê°„ í•„ìš”\n",
    "- Colab Pro ë˜ëŠ” GPU í™˜ê²½ ê¶Œì¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
